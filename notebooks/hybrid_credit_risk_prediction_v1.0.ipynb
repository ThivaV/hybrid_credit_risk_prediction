{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Credit Risk Prediction with Reinforcement Learning and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "![image.png](../docs/img/data_dictonary_description.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('../data/master_data/GiveMeSomeCredit.zip')\n",
    "\n",
    "df_cs_train = pd.read_csv(zf.open('cs-training.csv'))\n",
    "df_cs_test = pd.read_csv(zf.open('cs-test.csv'))\n",
    "df_sample_entry = pd.read_csv(zf.open('sampleEntry.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cs_test.shape)\n",
    "df_cs_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cs_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataset\n",
    "print(df_cs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information on data types and missing values\n",
    "df_cs_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(df_cs_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(df_cs_train.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "df_cs_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values (mean imputation as an example)\n",
    "df_cs_train.fillna(df_cs_train.mean(), inplace=True)\n",
    "\n",
    "# optionally, drop rows/columns with a high percentage of missing data\n",
    "df_cs_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualize Target Variable (SeriousDlqin2yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check distribution of target variable\n",
    "# sns.countplot(df_cs_train['SeriousDlqin2yrs'])\n",
    "# plt.title('Distribution of Target Variable (SeriousDlqin2yrs)')\n",
    "# plt.show()\n",
    "\n",
    "# # calculate percentage of default vs non-default\n",
    "# df_cs_train['SeriousDlqin2yrs'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_cs_train.corr(), annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions of numerical features\n",
    "df_cs_train.hist(bins=20, figsize=(20, 15), color='blue', edgecolor='black')\n",
    "plt.suptitle('Histograms of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# alternative: Plot KDE (Kernel Density Estimation) plots\n",
    "for column in df_cs_train.columns:\n",
    "    if df_cs_train[column].dtype != 'object':\n",
    "        sns.kdeplot(df_cs_train[column], shade=True)\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Boxplots to Detect Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots of numerical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=df_cs_train.drop(columns=['SeriousDlqin2yrs']), palette='Set3')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplots for Outlier Detection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Bivariate Analysis (Target vs Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot to check feature distribution by target\n",
    "for column in df_cs_train.columns:\n",
    "    if df_cs_train[column].dtype != 'object' and column != 'SeriousDlqin2yrs':\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.boxplot(x='SeriousDlqin2yrs', y=column, data=df_cs_train)\n",
    "        plt.title(f'Boxplot of {column} vs SeriousDlqin2yrs')\n",
    "        plt.show()\n",
    "\n",
    "# KDE plots to see how the distribution varies between classes\n",
    "for column in df_cs_train.columns:\n",
    "    if df_cs_train[column].dtype != 'object' and column != 'SeriousDlqin2yrs':\n",
    "        sns.kdeplot(df_cs_train[df_cs_train['SeriousDlqin2yrs'] == 0][column], label='No Default')\n",
    "        sns.kdeplot(df_cs_train[df_cs_train['SeriousDlqin2yrs'] == 1][column], label='Default')\n",
    "        plt.title(f'Distribution of {column} by Default Status')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Pairwise Relationships (Pair Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample pair plot (for a few features due to performance limitations)\n",
    "sns.pairplot(df_cs_train[['RevolvingUtilizationOfUnsecuredLines', 'age', 'DebtRatio', 'MonthlyIncome', 'SeriousDlqin2yrs']], hue='SeriousDlqin2yrs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using IQR (Interquartile Range)\n",
    "Q1 = df_cs_train.quantile(0.25)\n",
    "Q3 = df_cs_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Remove rows with outliers\n",
    "df_out = df_cs_train[~((df_cs_train < (Q1 - 1.5 * IQR)) | (df_cs_train > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Feature Engineering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feature creation\n",
    "df_cs_train['DebtToIncomeRatio'] = df_cs_train['DebtRatio'] / df_cs_train['MonthlyIncome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Feature Importance (Using Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # type: ignore\n",
    "\n",
    "# Prepare the data\n",
    "X = df_cs_train.drop(columns=['SeriousDlqin2yrs'])\n",
    "y = df_cs_train['SeriousDlqin2yrs']\n",
    "\n",
    "# Train a simple Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X.fillna(0), y)\n",
    "\n",
    "# Plot feature importances\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.title('Top 10 Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Insights and Conclusions\n",
    "\n",
    "- Summarize your insights:\n",
    "    - Which features are most correlated with defaulting?\n",
    "    - Are there any significant outliers or patterns?\n",
    "    - How imbalanced is the target variable?\n",
    "    - What relationships exist between features and the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Address Imbalance in Target Variable\n",
    "\n",
    "Since the dataset is often imbalanced, consider techniques such as SMOTE (Synthetic Minority Over-sampling Technique) or class weights during modeling to address the imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Notes\n",
    "\n",
    "- EDA is not a one-size-fits-all process. Always adjust the analysis based on the findings in each step.\n",
    "- You can also consider applying PCA (Principal Component Analysis) or t-SNE for dimensionality reduction and visualizing patterns in high-dimensional data.\n",
    "\n",
    "This comprehensive EDA will give you a deep understanding of the Give Me Some Credit dataset and prepare you for modeling tasks like credit risk prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check the Class Distribution\n",
    "\n",
    "If one class has significantly more samples than the other, you have an imbalanced dataset. \n",
    "For instance, if you see something like:\n",
    "- Class 0 (non-delinquent): 93%\n",
    "- Class 1 (delinquent): 7%\n",
    "\n",
    "This is a sign of imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of the target variable\n",
    "sns.countplot(x='SeriousDlqin2yrs', data=df_cs_train)\n",
    "plt.title('Class Distribution of SeriousDlqin2yrs')\n",
    "plt.show()\n",
    "\n",
    "# Percentage of each class\n",
    "class_distribution = df_cs_train['SeriousDlqin2yrs'].value_counts(normalize=True) * 100\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Metrics to Evaluate Imbalance\n",
    "\n",
    "- Class Distribution: Directly see the percentage of each class using .value_counts().\n",
    "- Class Ratios: Calculate the ratio of minority to majority class to quantify the imbalance. A ratio close to 0 indicates a severe imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class = df_cs_train['SeriousDlqin2yrs'].value_counts()[1]\n",
    "majority_class = df_cs_train['SeriousDlqin2yrs'].value_counts()[0]\n",
    "\n",
    "imbalance_ratio = minority_class / majority_class\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize Class Distribution in Features\n",
    "\n",
    "Use pair plots, histograms, or KDE plots to compare feature distributions between the two classes (delinquent vs. non-delinquent). \n",
    "This can help in visualizing patterns and understanding whether certain features show clear separations between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot for numerical features\n",
    "for column in df_cs_train.columns:\n",
    "    if df_cs_train[column].dtype != 'object' and column != 'SeriousDlqin2yrs':\n",
    "        sns.kdeplot(df_cs_train[df_cs_train['SeriousDlqin2yrs'] == 0][column], label='No Default', shade=True)\n",
    "        sns.kdeplot(df_cs_train[df_cs_train['SeriousDlqin2yrs'] == 1][column], label='Default', shade=True)\n",
    "        plt.title(f'Distribution of {column} by Class')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation Analysis\n",
    "\n",
    "Perform a correlation analysis to check whether any features are highly correlated with the target variable. \n",
    "Features with strong correlation to SeriousDlqin2yrs may help differentiate between the classes despite the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (including target variable)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_cs_train.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix with SeriousDlqin2yrs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Handling Imbalance\n",
    "\n",
    "Once you've identified the imbalance, you need strategies to handle it before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Resampling Techniques\n",
    "\n",
    "- Oversampling the Minority Class (SMOTE): Synthetic Minority Over-sampling Technique generates synthetic samples for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE # type: ignore\n",
    "\n",
    "X = df_cs_train.drop(columns='SeriousDlqin2yrs')\n",
    "y = df_cs_train['SeriousDlqin2yrs']\n",
    "\n",
    "smote = SMOTE()\n",
    "X_res, y_res = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Undersampling the Majority Class: Randomly remove samples from the majority class to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # type: ignore\n",
    "\n",
    "undersample = RandomUnderSampler()\n",
    "X_res, y_res = undersample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combination of Oversampling and Undersampling: A balanced approach where both the minority class is oversampled, and the majority class is undersampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Use Class Weights\n",
    "\n",
    "Most machine learning algorithms, like Random Forests and Logistic Regression, allow you to set class weights to give more importance to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier     # type: ignore\n",
    "from sklearn.model_selection import train_test_split    # type: ignore\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Set class weights to 'balanced'\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Evaluation Metrics Beyond Accuracy\n",
    "\n",
    "In imbalanced datasets, accuracy may not be a reliable metric since predicting the majority class can lead to high accuracy but poor performance on the minority class. Use metrics like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision, Recall, F1-Score: Precision measures how many selected items are relevant, while recall measures how many relevant items are selected. The F1-score balances these two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report   # type: ignore\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC-ROC (Area Under the Receiver Operating Characteristic Curve): Measures how well the model separates the classes. It’s especially useful for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score   # type: ignore\n",
    "\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'AUC-ROC Score: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cross-Validation with Stratified K-Folds\n",
    "\n",
    "When performing cross-validation, use stratified k-fold cross-validation to ensure that each fold has the same proportion of classes, preserving the imbalance ratio during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # type: ignore\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    ## Train your model here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Steps for EDA on Imbalanced Data:\n",
    "\n",
    "1. Identify the imbalance in the target variable using countplot and value_counts.\n",
    "2. Visualize relationships between the features and the target variable to understand feature distributions in different classes.\n",
    "3. Correlation analysis to find relationships between features and the target variable.\n",
    "4. Handle the imbalance using techniques like SMOTE, undersampling, or adjusting class weights.\n",
    "5. Use appropriate metrics such as precision, recall, F1-score, and AUC-ROC to evaluate the model on imbalanced data.\n",
    "6. Cross-validation with stratified sampling ensures that the imbalance is preserved in all training and validation folds.\n",
    "\n",
    "This approach will help you thoroughly explore the dataset, handle imbalanced data, and guide your modeling choices effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the important features\n",
    "\n",
    "To identify important features for building an LSTM model using the Give Me Some Credit dataset, it's important to analyze which features contribute most to predicting the target variable (SeriousDlqin2yrs). \n",
    "Here’s a step-by-step guide to identify the important features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Correlation Matrix\n",
    "\n",
    "A simple first step is to compute the correlation between features and the target variable. This will give you a sense of how strongly individual features are associated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df_cs_train.corr()\n",
    "\n",
    "# Plot the heatmap for the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix with SeriousDlqin2yrs')\n",
    "plt.show()\n",
    "\n",
    "# Check correlation with the target variable 'SeriousDlqin2yrs'\n",
    "target_corr = corr_matrix['SeriousDlqin2yrs'].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for features with a higher absolute correlation with the target variable. These may be strong candidates to include in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Importance using Random Forest\n",
    "\n",
    "Although LSTM models don’t inherently provide feature importance, you can use other models like Random Forest to compute feature importance scores and then feed the most important features into your LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # type: ignore\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df_cs_train.drop(columns='SeriousDlqin2yrs')\n",
    "y = df_cs_train['SeriousDlqin2yrs']\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "importance = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importance})\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help you identify the top features to focus on for your LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recursive Feature Elimination (RFE)\n",
    "\n",
    "Recursive Feature Elimination (RFE) helps in selecting the most important features by recursively removing less significant features. \n",
    "Although it is slower than Random Forest, it systematically eliminates features based on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE           # type: ignore\n",
    "from sklearn.linear_model import LogisticRegression # type: ignore\n",
    "\n",
    "# Initialize a logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Use RFE for feature selection\n",
    "# Choose top 10 features\n",
    "rfe = RFE(logreg, n_features_to_select=10)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print('Selected features:', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP values provide interpretable machine learning insights by showing how much each feature contributes to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap # type: ignore\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values[1], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHAP summary plot will show you which features contribute the most to the model’s predictions, which can guide your feature selection for the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Statistical Methods (Chi-Square Test for Categorical Features)\n",
    "\n",
    "If there are any categorical features (not common in this dataset, but if added in preprocessing), you can use statistical tests like Chi-Square to determine feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2      # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler  # type: ignore\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compute Chi-Square scores\n",
    "chi_scores = chi2(X_scaled, y)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "chi2_importance = pd.DataFrame({'feature': X.columns, 'chi2_score': chi_scores[0]})\n",
    "chi2_importance = chi2_importance.sort_values(by='chi2_score', ascending=False)\n",
    "\n",
    "# Plot the Chi-Square scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='chi2_score', y='feature', data=chi2_importance)\n",
    "plt.title('Chi-Square Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using Autoencoders for Feature Extraction\n",
    "\n",
    "Autoencoders are neural networks that can be used to automatically learn the most relevant features by compressing and reconstructing the data. These compressed representations can then be used as input to your LSTM model.\n",
    "\n",
    "Steps to Consider for LSTM Feature Selection:\n",
    "1. Correlation matrix to find initial relationships between features and the target.\n",
    "2. Random Forest feature importance to prioritize features that may have a higher predictive power.\n",
    "3. RFE to systematically reduce the number of features.\n",
    "4. SHAP values for interpretability of feature impact.\n",
    "5. Statistical tests for further feature selection (if applicable).\n",
    "6. Autoencoders for unsupervised feature learning.\n",
    "\n",
    "**Final Considerations:**\n",
    "\n",
    "After identifying the top features, you may want to normalize the data and possibly use PCA (Principal Component Analysis) for dimensionality reduction before feeding the data into the LSTM model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
